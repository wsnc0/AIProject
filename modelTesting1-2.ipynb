{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer(iterable:list|dict):\n",
    "    \"\"\"\n",
    "    Displays Lists and Dictionaries nicely\n",
    "    \"\"\"\n",
    "    if isinstance(iterable,list):\n",
    "        # print(\"List\".center(16,\"-\"))\n",
    "        for item in iterable:\n",
    "            print(f\" - {item}\")\n",
    "        print()\n",
    "    elif isinstance(iterable,dict):\n",
    "        # print(\"Dictionary\".center(16,\"-\"))\n",
    "        \n",
    "        for key in list(iterable.keys()):\n",
    "            print(f\" {key} :\",iterable[key])\n",
    "        print()\n",
    "    else:\n",
    "        print(iterable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def getClassNames(path = 'processedData/train'):\n",
    "    folderNames = [folder for folder in os.listdir(path) if os.path.isdir(os.path.join(path, folder))]\n",
    "    cleanedNames = []\n",
    "    for idx in range(len(folderNames)):\n",
    "        words = folderNames[idx].split()\n",
    "        words.pop(0)\n",
    "        words.pop()\n",
    "        if \"-\" in words:\n",
    "            words.pop(words.index(\"-\"))\n",
    "\n",
    "        out = \" \".join(words)\n",
    "        cleanedNames.append(out)\n",
    "    classMapping = {}\n",
    "    for index, value in enumerate(cleanedNames):\n",
    "        classMapping[index] = value\n",
    "    print(classMapping)\n",
    "\n",
    "    return classMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveMeImageSize(modelVersion = \"B4\") -> tuple:\n",
    "    \"\"\"\n",
    "    Returns the resolution for the respective EfficientNet model verisons:\n",
    "\n",
    "    \"B0\", \"B1\", \"B2\", \"B3\", \"B4\", \"B\"5, \"B6\", \"B7\"\n",
    "\n",
    "    \"\"\"\n",
    "    if modelVersion == \"B0\":\n",
    "        return (224,224)\n",
    "    elif modelVersion == \"B1\":\n",
    "        return (240,240)\n",
    "    elif modelVersion == \"B2\":\n",
    "        return (260,260)\n",
    "    elif modelVersion == \"B3\":\n",
    "        return (300,300)\n",
    "    elif modelVersion == \"B4\":\n",
    "        return (380,380)\n",
    "    elif modelVersion == \"B5\":\n",
    "        return (456,456)\n",
    "    elif modelVersion == \"B6\":\n",
    "        return (528,528)\n",
    "    elif modelVersion == \"B7\":\n",
    "        return (600,600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "#? function for image transformation\n",
    "def pad_to_Square(image):\n",
    "    width, height = image.size\n",
    "    max_side = max(width,height)\n",
    "    left_pad = (max_side-width)//2\n",
    "    right_pad = (max_side-width-left_pad)\n",
    "    top_pad = (max_side-height)//2\n",
    "    bot_pad = (max_side-height - top_pad)\n",
    "\n",
    "    padded_image = transforms.functional.pad(image, (left_pad, top_pad, right_pad, bot_pad), padding_mode='constant', fill=0)\n",
    "    return padded_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "# Function to create the necessary directories\n",
    "def create_dirs(base_dir, classes):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    for subset in ['train', 'valid', 'test']:\n",
    "        subset_path = os.path.join(base_dir, subset)\n",
    "        os.makedirs(subset_path, exist_ok=True)\n",
    "        for class_name in classes:\n",
    "            os.makedirs(os.path.join(subset_path, class_name), exist_ok=True)\n",
    "\n",
    "# Function to split and save images into train and test directories\n",
    "def split_and_process_images(raw_data_dir:str, processed_data_dir:str, randomState=42):\n",
    "    # Get all class names (subfolder names)\n",
    "    classes = os.listdir(raw_data_dir)\n",
    "    print(\"Subfolders:\",classes)\n",
    "    create_dirs(processed_data_dir, classes)\n",
    "    \n",
    "    # Iterate through each class folder\n",
    "    for class_name in classes:\n",
    "        class_folder = os.path.join(raw_data_dir, class_name)\n",
    "        if os.path.isdir(class_folder):\n",
    "            # Get all image filenames\n",
    "            image_filenames = os.listdir(class_folder)\n",
    "            print(f\"{class_name} : {len(image_filenames)} files.\")\n",
    "            # Split into train and test sets\n",
    "            train_files, testval_files = train_test_split(image_filenames, test_size=0.2, random_state=randomState)\n",
    "            print(f\" Train: {len(train_files)}, Test&Val: {len(testval_files)}\")\n",
    "            # Process and save training images\n",
    "            for filename in train_files:\n",
    "                image_path = os.path.join(class_folder, filename)\n",
    "                img = Image.open(image_path)\n",
    "                # Save the training image\n",
    "                train_save_path = os.path.join(processed_data_dir, 'train', class_name, filename)\n",
    "                img.save(train_save_path)\n",
    "\n",
    "            # Split into validation and test sets\n",
    "            val_files, test_files = train_test_split(testval_files, test_size=0.5, random_state=randomState)\n",
    "\n",
    "            # Process and save validation images\n",
    "            for filename in val_files:\n",
    "                image_path = os.path.join(class_folder, filename)\n",
    "                img = Image.open(image_path)\n",
    "                # Save the validation image\n",
    "                val_save_path = os.path.join(processed_data_dir, 'valid', class_name, filename)\n",
    "                img.save(val_save_path)\n",
    "\n",
    "            # Process and save test images\n",
    "            for filename in test_files:\n",
    "                image_path = os.path.join(class_folder, filename)\n",
    "                img = Image.open(image_path)\n",
    "                # Save the testing image\n",
    "                test_save_path = os.path.join(processed_data_dir, 'test', class_name, filename)\n",
    "                img.save(test_save_path)\n",
    "\n",
    "        else:\n",
    "            print(f\"Error on {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showGrads(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Layer: {name}\".ljust(46), f\"requires_grad: {param.requires_grad}\")\n",
    "\n",
    "def freeze_model(model):\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad=False\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad = True \n",
    "    \n",
    "def unfreeze_last_n_blocks(model,n):\n",
    "    lastblock = len(model.features)-1\n",
    "    blocknames = [\"classifier\"] + [f\"features.{lastblock-i}\" for i in range(n)]\n",
    "    for name,params in model.named_parameters():\n",
    "        if any(substring in name for substring in blocknames): \n",
    "            params.requires_grad = True\n",
    "\n",
    "    print(f\"Layers unfrozen: {blocknames}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders and Processing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning folder names into class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 380)\n"
     ]
    }
   ],
   "source": [
    "imgSize = giveMeImageSize()\n",
    "print(imgSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Mean and STD for image normalization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectMean = False  #? <- Update Mean and STD if model is chagned or image is resized\n",
    "\n",
    "\n",
    "if collectMean:\n",
    "    #? Getting the mean and std of the raw dataset\n",
    "    initial_Transform = transforms.Compose([\n",
    "        transforms.Lambda(pad_to_Square),\n",
    "        transforms.Resize(imgSize),  # Resize images\n",
    "        transforms.ToTensor(),  # Convert images to tensor\n",
    "    ])\n",
    "\n",
    "    #? raw dataset File path\n",
    "    raw_path = './data'\n",
    "\n",
    "    # Load raw dataset\n",
    "    dataset = datasets.ImageFolder(root=raw_path, transform=initial_Transform)\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=False)       #? num_workers indicate the number of parallel processes\n",
    "\n",
    "    # Initialize sums\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images = 0\n",
    "\n",
    "    print(\"dataset:\", dataset)\n",
    "    batch = 0\n",
    "    print(f\"Total Batches: {len(loader)}\")\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)  # batch size\n",
    "        images = images.view(batch_samples, images.size(1), -1)  # flatten H and W\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images += batch_samples\n",
    "        batch += 1\n",
    "        print(f\"Batch [{batch}/{len(loader)}]\".ljust(20),\n",
    "            f\"Mean: {(mean/total_images)}\",\n",
    "            f\"STD: {(std/total_images)}\", end=\"\\r\")\n",
    "    print()\n",
    "\n",
    "    mean /= total_images\n",
    "    std /= total_images\n",
    "\n",
    "    print(f\"Mean: {mean}\")\n",
    "    print(f\"Std: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Datasets:\n",
    "- Training DataSet with augmentation.\n",
    "- Testing DataSet without augmentation.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " #? Copy form previous cell outputs\n",
    "mean = torch.tensor([0.5246, 0.3975, 0.3844])\n",
    "std = torch.tensor([0.2752, 0.2196, 0.2174])\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Lambda(pad_to_Square),\n",
    "    transforms.Resize(imgSize),  # Resize images\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize(mean=mean, std=std),  # Normalize images\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Lambda(pad_to_Square),\n",
    "    transforms.Resize(imgSize),  # Resize images\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 21719\n",
      "    Root location: processedData/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Lambda()\n",
      "               Resize(size=(380, 380), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "               Normalize(mean=tensor([0.5246, 0.3975, 0.3844]), std=tensor([0.2752, 0.2196, 0.2174]))\n",
      "           ) Batches: 340 \n",
      "-------------------------------------------------\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 2715\n",
      "    Root location: processedData/valid\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Lambda()\n",
      "               Resize(size=(380, 380), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           ) Batches: 43 \n",
      "-------------------------------------------------\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 2719\n",
      "    Root location: processedData/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Lambda()\n",
      "               Resize(size=(380, 380), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           ) Batches: 43 \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#? training dataset directory\n",
    "train_path = \"processedData/train\"\n",
    "#? validation dataset directory\n",
    "valid_path = \"processedData/valid\"\n",
    "#? testing dataset directory\n",
    "test_path = \"processedData/test\"\n",
    "\n",
    "batchSize = 64\n",
    "if train_path:\n",
    "    #? Load TRAIN dataset from directory\n",
    "    train_dataset = datasets.ImageFolder(root=train_path, transform=transform_train)\n",
    "    #? Create a DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "    print(train_dataset, f\"Batches: {len(train_dataloader)}\",\"\\n\".ljust(50,\"-\"))\n",
    "\n",
    "if valid_path:\n",
    "    #? Load VALID dataset from directory\n",
    "    valid_dataset = datasets.ImageFolder(root=valid_path, transform=transform_test)\n",
    "    #? Create a DataLoader\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batchSize, shuffle=False)\n",
    "    print(valid_dataset, f\"Batches: {len(valid_dataloader)}\",\"\\n\".ljust(50,\"-\"))\n",
    "\n",
    "if test_path:\n",
    "    #? Load TEST dataset from directory\n",
    "    test_dataset = datasets.ImageFolder(root=test_path, transform=transform_test)\n",
    "    #? Create a DataLoader\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False)\n",
    "    print(test_dataset, f\"Batches: {len(test_dataloader)}\",\"\\n\".ljust(50,\"-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.0.weight                     requires_grad: False\n",
      "Layer: features.0.1.weight                     requires_grad: False\n",
      "Layer: features.0.1.bias                       requires_grad: False\n",
      "Layer: features.1.0.block.0.0.weight           requires_grad: False\n",
      "Layer: features.1.0.block.0.1.weight           requires_grad: False\n",
      "Layer: features.1.0.block.0.1.bias             requires_grad: False\n",
      "Layer: features.1.0.block.1.fc1.weight         requires_grad: False\n",
      "Layer: features.1.0.block.1.fc1.bias           requires_grad: False\n",
      "Layer: features.1.0.block.1.fc2.weight         requires_grad: False\n",
      "Layer: features.1.0.block.1.fc2.bias           requires_grad: False\n",
      "Layer: features.1.0.block.2.0.weight           requires_grad: False\n",
      "Layer: features.1.0.block.2.1.weight           requires_grad: False\n",
      "Layer: features.1.0.block.2.1.bias             requires_grad: False\n",
      "Layer: features.1.1.block.0.0.weight           requires_grad: False\n",
      "Layer: features.1.1.block.0.1.weight           requires_grad: False\n",
      "Layer: features.1.1.block.0.1.bias             requires_grad: False\n",
      "Layer: features.1.1.block.1.fc1.weight         requires_grad: False\n",
      "Layer: features.1.1.block.1.fc1.bias           requires_grad: False\n",
      "Layer: features.1.1.block.1.fc2.weight         requires_grad: False\n",
      "Layer: features.1.1.block.1.fc2.bias           requires_grad: False\n",
      "Layer: features.1.1.block.2.0.weight           requires_grad: False\n",
      "Layer: features.1.1.block.2.1.weight           requires_grad: False\n",
      "Layer: features.1.1.block.2.1.bias             requires_grad: False\n",
      "Layer: features.2.0.block.0.0.weight           requires_grad: False\n",
      "Layer: features.2.0.block.0.1.weight           requires_grad: False\n",
      "Layer: features.2.0.block.0.1.bias             requires_grad: False\n",
      "Layer: features.2.0.block.1.0.weight           requires_grad: False\n",
      "Layer: features.2.0.block.1.1.weight           requires_grad: False\n",
      "Layer: features.2.0.block.1.1.bias             requires_grad: False\n",
      "Layer: features.2.0.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.2.0.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.2.0.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.2.0.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.2.0.block.3.0.weight           requires_grad: False\n",
      "Layer: features.2.0.block.3.1.weight           requires_grad: False\n",
      "Layer: features.2.0.block.3.1.bias             requires_grad: False\n",
      "Layer: features.2.1.block.0.0.weight           requires_grad: False\n",
      "Layer: features.2.1.block.0.1.weight           requires_grad: False\n",
      "Layer: features.2.1.block.0.1.bias             requires_grad: False\n",
      "Layer: features.2.1.block.1.0.weight           requires_grad: False\n",
      "Layer: features.2.1.block.1.1.weight           requires_grad: False\n",
      "Layer: features.2.1.block.1.1.bias             requires_grad: False\n",
      "Layer: features.2.1.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.2.1.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.2.1.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.2.1.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.2.1.block.3.0.weight           requires_grad: False\n",
      "Layer: features.2.1.block.3.1.weight           requires_grad: False\n",
      "Layer: features.2.1.block.3.1.bias             requires_grad: False\n",
      "Layer: features.2.2.block.0.0.weight           requires_grad: False\n",
      "Layer: features.2.2.block.0.1.weight           requires_grad: False\n",
      "Layer: features.2.2.block.0.1.bias             requires_grad: False\n",
      "Layer: features.2.2.block.1.0.weight           requires_grad: False\n",
      "Layer: features.2.2.block.1.1.weight           requires_grad: False\n",
      "Layer: features.2.2.block.1.1.bias             requires_grad: False\n",
      "Layer: features.2.2.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.2.2.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.2.2.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.2.2.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.2.2.block.3.0.weight           requires_grad: False\n",
      "Layer: features.2.2.block.3.1.weight           requires_grad: False\n",
      "Layer: features.2.2.block.3.1.bias             requires_grad: False\n",
      "Layer: features.2.3.block.0.0.weight           requires_grad: False\n",
      "Layer: features.2.3.block.0.1.weight           requires_grad: False\n",
      "Layer: features.2.3.block.0.1.bias             requires_grad: False\n",
      "Layer: features.2.3.block.1.0.weight           requires_grad: False\n",
      "Layer: features.2.3.block.1.1.weight           requires_grad: False\n",
      "Layer: features.2.3.block.1.1.bias             requires_grad: False\n",
      "Layer: features.2.3.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.2.3.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.2.3.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.2.3.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.2.3.block.3.0.weight           requires_grad: False\n",
      "Layer: features.2.3.block.3.1.weight           requires_grad: False\n",
      "Layer: features.2.3.block.3.1.bias             requires_grad: False\n",
      "Layer: features.3.0.block.0.0.weight           requires_grad: False\n",
      "Layer: features.3.0.block.0.1.weight           requires_grad: False\n",
      "Layer: features.3.0.block.0.1.bias             requires_grad: False\n",
      "Layer: features.3.0.block.1.0.weight           requires_grad: False\n",
      "Layer: features.3.0.block.1.1.weight           requires_grad: False\n",
      "Layer: features.3.0.block.1.1.bias             requires_grad: False\n",
      "Layer: features.3.0.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.3.0.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.3.0.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.3.0.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.3.0.block.3.0.weight           requires_grad: False\n",
      "Layer: features.3.0.block.3.1.weight           requires_grad: False\n",
      "Layer: features.3.0.block.3.1.bias             requires_grad: False\n",
      "Layer: features.3.1.block.0.0.weight           requires_grad: False\n",
      "Layer: features.3.1.block.0.1.weight           requires_grad: False\n",
      "Layer: features.3.1.block.0.1.bias             requires_grad: False\n",
      "Layer: features.3.1.block.1.0.weight           requires_grad: False\n",
      "Layer: features.3.1.block.1.1.weight           requires_grad: False\n",
      "Layer: features.3.1.block.1.1.bias             requires_grad: False\n",
      "Layer: features.3.1.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.3.1.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.3.1.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.3.1.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.3.1.block.3.0.weight           requires_grad: False\n",
      "Layer: features.3.1.block.3.1.weight           requires_grad: False\n",
      "Layer: features.3.1.block.3.1.bias             requires_grad: False\n",
      "Layer: features.3.2.block.0.0.weight           requires_grad: False\n",
      "Layer: features.3.2.block.0.1.weight           requires_grad: False\n",
      "Layer: features.3.2.block.0.1.bias             requires_grad: False\n",
      "Layer: features.3.2.block.1.0.weight           requires_grad: False\n",
      "Layer: features.3.2.block.1.1.weight           requires_grad: False\n",
      "Layer: features.3.2.block.1.1.bias             requires_grad: False\n",
      "Layer: features.3.2.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.3.2.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.3.2.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.3.2.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.3.2.block.3.0.weight           requires_grad: False\n",
      "Layer: features.3.2.block.3.1.weight           requires_grad: False\n",
      "Layer: features.3.2.block.3.1.bias             requires_grad: False\n",
      "Layer: features.3.3.block.0.0.weight           requires_grad: False\n",
      "Layer: features.3.3.block.0.1.weight           requires_grad: False\n",
      "Layer: features.3.3.block.0.1.bias             requires_grad: False\n",
      "Layer: features.3.3.block.1.0.weight           requires_grad: False\n",
      "Layer: features.3.3.block.1.1.weight           requires_grad: False\n",
      "Layer: features.3.3.block.1.1.bias             requires_grad: False\n",
      "Layer: features.3.3.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.3.3.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.3.3.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.3.3.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.3.3.block.3.0.weight           requires_grad: False\n",
      "Layer: features.3.3.block.3.1.weight           requires_grad: False\n",
      "Layer: features.3.3.block.3.1.bias             requires_grad: False\n",
      "Layer: features.4.0.block.0.0.weight           requires_grad: False\n",
      "Layer: features.4.0.block.0.1.weight           requires_grad: False\n",
      "Layer: features.4.0.block.0.1.bias             requires_grad: False\n",
      "Layer: features.4.0.block.1.0.weight           requires_grad: False\n",
      "Layer: features.4.0.block.1.1.weight           requires_grad: False\n",
      "Layer: features.4.0.block.1.1.bias             requires_grad: False\n",
      "Layer: features.4.0.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.4.0.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.4.0.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.4.0.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.4.0.block.3.0.weight           requires_grad: False\n",
      "Layer: features.4.0.block.3.1.weight           requires_grad: False\n",
      "Layer: features.4.0.block.3.1.bias             requires_grad: False\n",
      "Layer: features.4.1.block.0.0.weight           requires_grad: False\n",
      "Layer: features.4.1.block.0.1.weight           requires_grad: False\n",
      "Layer: features.4.1.block.0.1.bias             requires_grad: False\n",
      "Layer: features.4.1.block.1.0.weight           requires_grad: False\n",
      "Layer: features.4.1.block.1.1.weight           requires_grad: False\n",
      "Layer: features.4.1.block.1.1.bias             requires_grad: False\n",
      "Layer: features.4.1.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.4.1.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.4.1.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.4.1.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.4.1.block.3.0.weight           requires_grad: False\n",
      "Layer: features.4.1.block.3.1.weight           requires_grad: False\n",
      "Layer: features.4.1.block.3.1.bias             requires_grad: False\n",
      "Layer: features.4.2.block.0.0.weight           requires_grad: False\n",
      "Layer: features.4.2.block.0.1.weight           requires_grad: False\n",
      "Layer: features.4.2.block.0.1.bias             requires_grad: False\n",
      "Layer: features.4.2.block.1.0.weight           requires_grad: False\n",
      "Layer: features.4.2.block.1.1.weight           requires_grad: False\n",
      "Layer: features.4.2.block.1.1.bias             requires_grad: False\n",
      "Layer: features.4.2.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.4.2.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.4.2.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.4.2.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.4.2.block.3.0.weight           requires_grad: False\n",
      "Layer: features.4.2.block.3.1.weight           requires_grad: False\n",
      "Layer: features.4.2.block.3.1.bias             requires_grad: False\n",
      "Layer: features.4.3.block.0.0.weight           requires_grad: False\n",
      "Layer: features.4.3.block.0.1.weight           requires_grad: False\n",
      "Layer: features.4.3.block.0.1.bias             requires_grad: False\n",
      "Layer: features.4.3.block.1.0.weight           requires_grad: False\n",
      "Layer: features.4.3.block.1.1.weight           requires_grad: False\n",
      "Layer: features.4.3.block.1.1.bias             requires_grad: False\n",
      "Layer: features.4.3.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.4.3.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.4.3.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.4.3.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.4.3.block.3.0.weight           requires_grad: False\n",
      "Layer: features.4.3.block.3.1.weight           requires_grad: False\n",
      "Layer: features.4.3.block.3.1.bias             requires_grad: False\n",
      "Layer: features.4.4.block.0.0.weight           requires_grad: False\n",
      "Layer: features.4.4.block.0.1.weight           requires_grad: False\n",
      "Layer: features.4.4.block.0.1.bias             requires_grad: False\n",
      "Layer: features.4.4.block.1.0.weight           requires_grad: False\n",
      "Layer: features.4.4.block.1.1.weight           requires_grad: False\n",
      "Layer: features.4.4.block.1.1.bias             requires_grad: False\n",
      "Layer: features.4.4.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.4.4.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.4.4.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.4.4.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.4.4.block.3.0.weight           requires_grad: False\n",
      "Layer: features.4.4.block.3.1.weight           requires_grad: False\n",
      "Layer: features.4.4.block.3.1.bias             requires_grad: False\n",
      "Layer: features.4.5.block.0.0.weight           requires_grad: False\n",
      "Layer: features.4.5.block.0.1.weight           requires_grad: False\n",
      "Layer: features.4.5.block.0.1.bias             requires_grad: False\n",
      "Layer: features.4.5.block.1.0.weight           requires_grad: False\n",
      "Layer: features.4.5.block.1.1.weight           requires_grad: False\n",
      "Layer: features.4.5.block.1.1.bias             requires_grad: False\n",
      "Layer: features.4.5.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.4.5.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.4.5.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.4.5.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.4.5.block.3.0.weight           requires_grad: False\n",
      "Layer: features.4.5.block.3.1.weight           requires_grad: False\n",
      "Layer: features.4.5.block.3.1.bias             requires_grad: False\n",
      "Layer: features.5.0.block.0.0.weight           requires_grad: False\n",
      "Layer: features.5.0.block.0.1.weight           requires_grad: False\n",
      "Layer: features.5.0.block.0.1.bias             requires_grad: False\n",
      "Layer: features.5.0.block.1.0.weight           requires_grad: False\n",
      "Layer: features.5.0.block.1.1.weight           requires_grad: False\n",
      "Layer: features.5.0.block.1.1.bias             requires_grad: False\n",
      "Layer: features.5.0.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.5.0.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.5.0.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.5.0.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.5.0.block.3.0.weight           requires_grad: False\n",
      "Layer: features.5.0.block.3.1.weight           requires_grad: False\n",
      "Layer: features.5.0.block.3.1.bias             requires_grad: False\n",
      "Layer: features.5.1.block.0.0.weight           requires_grad: False\n",
      "Layer: features.5.1.block.0.1.weight           requires_grad: False\n",
      "Layer: features.5.1.block.0.1.bias             requires_grad: False\n",
      "Layer: features.5.1.block.1.0.weight           requires_grad: False\n",
      "Layer: features.5.1.block.1.1.weight           requires_grad: False\n",
      "Layer: features.5.1.block.1.1.bias             requires_grad: False\n",
      "Layer: features.5.1.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.5.1.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.5.1.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.5.1.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.5.1.block.3.0.weight           requires_grad: False\n",
      "Layer: features.5.1.block.3.1.weight           requires_grad: False\n",
      "Layer: features.5.1.block.3.1.bias             requires_grad: False\n",
      "Layer: features.5.2.block.0.0.weight           requires_grad: False\n",
      "Layer: features.5.2.block.0.1.weight           requires_grad: False\n",
      "Layer: features.5.2.block.0.1.bias             requires_grad: False\n",
      "Layer: features.5.2.block.1.0.weight           requires_grad: False\n",
      "Layer: features.5.2.block.1.1.weight           requires_grad: False\n",
      "Layer: features.5.2.block.1.1.bias             requires_grad: False\n",
      "Layer: features.5.2.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.5.2.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.5.2.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.5.2.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.5.2.block.3.0.weight           requires_grad: False\n",
      "Layer: features.5.2.block.3.1.weight           requires_grad: False\n",
      "Layer: features.5.2.block.3.1.bias             requires_grad: False\n",
      "Layer: features.5.3.block.0.0.weight           requires_grad: False\n",
      "Layer: features.5.3.block.0.1.weight           requires_grad: False\n",
      "Layer: features.5.3.block.0.1.bias             requires_grad: False\n",
      "Layer: features.5.3.block.1.0.weight           requires_grad: False\n",
      "Layer: features.5.3.block.1.1.weight           requires_grad: False\n",
      "Layer: features.5.3.block.1.1.bias             requires_grad: False\n",
      "Layer: features.5.3.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.5.3.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.5.3.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.5.3.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.5.3.block.3.0.weight           requires_grad: False\n",
      "Layer: features.5.3.block.3.1.weight           requires_grad: False\n",
      "Layer: features.5.3.block.3.1.bias             requires_grad: False\n",
      "Layer: features.5.4.block.0.0.weight           requires_grad: False\n",
      "Layer: features.5.4.block.0.1.weight           requires_grad: False\n",
      "Layer: features.5.4.block.0.1.bias             requires_grad: False\n",
      "Layer: features.5.4.block.1.0.weight           requires_grad: False\n",
      "Layer: features.5.4.block.1.1.weight           requires_grad: False\n",
      "Layer: features.5.4.block.1.1.bias             requires_grad: False\n",
      "Layer: features.5.4.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.5.4.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.5.4.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.5.4.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.5.4.block.3.0.weight           requires_grad: False\n",
      "Layer: features.5.4.block.3.1.weight           requires_grad: False\n",
      "Layer: features.5.4.block.3.1.bias             requires_grad: False\n",
      "Layer: features.5.5.block.0.0.weight           requires_grad: False\n",
      "Layer: features.5.5.block.0.1.weight           requires_grad: False\n",
      "Layer: features.5.5.block.0.1.bias             requires_grad: False\n",
      "Layer: features.5.5.block.1.0.weight           requires_grad: False\n",
      "Layer: features.5.5.block.1.1.weight           requires_grad: False\n",
      "Layer: features.5.5.block.1.1.bias             requires_grad: False\n",
      "Layer: features.5.5.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.5.5.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.5.5.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.5.5.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.5.5.block.3.0.weight           requires_grad: False\n",
      "Layer: features.5.5.block.3.1.weight           requires_grad: False\n",
      "Layer: features.5.5.block.3.1.bias             requires_grad: False\n",
      "Layer: features.6.0.block.0.0.weight           requires_grad: False\n",
      "Layer: features.6.0.block.0.1.weight           requires_grad: False\n",
      "Layer: features.6.0.block.0.1.bias             requires_grad: False\n",
      "Layer: features.6.0.block.1.0.weight           requires_grad: False\n",
      "Layer: features.6.0.block.1.1.weight           requires_grad: False\n",
      "Layer: features.6.0.block.1.1.bias             requires_grad: False\n",
      "Layer: features.6.0.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.6.0.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.6.0.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.6.0.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.6.0.block.3.0.weight           requires_grad: False\n",
      "Layer: features.6.0.block.3.1.weight           requires_grad: False\n",
      "Layer: features.6.0.block.3.1.bias             requires_grad: False\n",
      "Layer: features.6.1.block.0.0.weight           requires_grad: False\n",
      "Layer: features.6.1.block.0.1.weight           requires_grad: False\n",
      "Layer: features.6.1.block.0.1.bias             requires_grad: False\n",
      "Layer: features.6.1.block.1.0.weight           requires_grad: False\n",
      "Layer: features.6.1.block.1.1.weight           requires_grad: False\n",
      "Layer: features.6.1.block.1.1.bias             requires_grad: False\n",
      "Layer: features.6.1.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.6.1.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.6.1.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.6.1.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.6.1.block.3.0.weight           requires_grad: False\n",
      "Layer: features.6.1.block.3.1.weight           requires_grad: False\n",
      "Layer: features.6.1.block.3.1.bias             requires_grad: False\n",
      "Layer: features.6.2.block.0.0.weight           requires_grad: False\n",
      "Layer: features.6.2.block.0.1.weight           requires_grad: False\n",
      "Layer: features.6.2.block.0.1.bias             requires_grad: False\n",
      "Layer: features.6.2.block.1.0.weight           requires_grad: False\n",
      "Layer: features.6.2.block.1.1.weight           requires_grad: False\n",
      "Layer: features.6.2.block.1.1.bias             requires_grad: False\n",
      "Layer: features.6.2.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.6.2.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.6.2.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.6.2.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.6.2.block.3.0.weight           requires_grad: False\n",
      "Layer: features.6.2.block.3.1.weight           requires_grad: False\n",
      "Layer: features.6.2.block.3.1.bias             requires_grad: False\n",
      "Layer: features.6.3.block.0.0.weight           requires_grad: False\n",
      "Layer: features.6.3.block.0.1.weight           requires_grad: False\n",
      "Layer: features.6.3.block.0.1.bias             requires_grad: False\n",
      "Layer: features.6.3.block.1.0.weight           requires_grad: False\n",
      "Layer: features.6.3.block.1.1.weight           requires_grad: False\n",
      "Layer: features.6.3.block.1.1.bias             requires_grad: False\n",
      "Layer: features.6.3.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.6.3.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.6.3.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.6.3.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.6.3.block.3.0.weight           requires_grad: False\n",
      "Layer: features.6.3.block.3.1.weight           requires_grad: False\n",
      "Layer: features.6.3.block.3.1.bias             requires_grad: False\n",
      "Layer: features.6.4.block.0.0.weight           requires_grad: False\n",
      "Layer: features.6.4.block.0.1.weight           requires_grad: False\n",
      "Layer: features.6.4.block.0.1.bias             requires_grad: False\n",
      "Layer: features.6.4.block.1.0.weight           requires_grad: False\n",
      "Layer: features.6.4.block.1.1.weight           requires_grad: False\n",
      "Layer: features.6.4.block.1.1.bias             requires_grad: False\n",
      "Layer: features.6.4.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.6.4.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.6.4.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.6.4.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.6.4.block.3.0.weight           requires_grad: False\n",
      "Layer: features.6.4.block.3.1.weight           requires_grad: False\n",
      "Layer: features.6.4.block.3.1.bias             requires_grad: False\n",
      "Layer: features.6.5.block.0.0.weight           requires_grad: False\n",
      "Layer: features.6.5.block.0.1.weight           requires_grad: False\n",
      "Layer: features.6.5.block.0.1.bias             requires_grad: False\n",
      "Layer: features.6.5.block.1.0.weight           requires_grad: False\n",
      "Layer: features.6.5.block.1.1.weight           requires_grad: False\n",
      "Layer: features.6.5.block.1.1.bias             requires_grad: False\n",
      "Layer: features.6.5.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.6.5.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.6.5.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.6.5.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.6.5.block.3.0.weight           requires_grad: False\n",
      "Layer: features.6.5.block.3.1.weight           requires_grad: False\n",
      "Layer: features.6.5.block.3.1.bias             requires_grad: False\n",
      "Layer: features.6.6.block.0.0.weight           requires_grad: False\n",
      "Layer: features.6.6.block.0.1.weight           requires_grad: False\n",
      "Layer: features.6.6.block.0.1.bias             requires_grad: False\n",
      "Layer: features.6.6.block.1.0.weight           requires_grad: False\n",
      "Layer: features.6.6.block.1.1.weight           requires_grad: False\n",
      "Layer: features.6.6.block.1.1.bias             requires_grad: False\n",
      "Layer: features.6.6.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.6.6.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.6.6.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.6.6.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.6.6.block.3.0.weight           requires_grad: False\n",
      "Layer: features.6.6.block.3.1.weight           requires_grad: False\n",
      "Layer: features.6.6.block.3.1.bias             requires_grad: False\n",
      "Layer: features.6.7.block.0.0.weight           requires_grad: False\n",
      "Layer: features.6.7.block.0.1.weight           requires_grad: False\n",
      "Layer: features.6.7.block.0.1.bias             requires_grad: False\n",
      "Layer: features.6.7.block.1.0.weight           requires_grad: False\n",
      "Layer: features.6.7.block.1.1.weight           requires_grad: False\n",
      "Layer: features.6.7.block.1.1.bias             requires_grad: False\n",
      "Layer: features.6.7.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.6.7.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.6.7.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.6.7.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.6.7.block.3.0.weight           requires_grad: False\n",
      "Layer: features.6.7.block.3.1.weight           requires_grad: False\n",
      "Layer: features.6.7.block.3.1.bias             requires_grad: False\n",
      "Layer: features.7.0.block.0.0.weight           requires_grad: False\n",
      "Layer: features.7.0.block.0.1.weight           requires_grad: False\n",
      "Layer: features.7.0.block.0.1.bias             requires_grad: False\n",
      "Layer: features.7.0.block.1.0.weight           requires_grad: False\n",
      "Layer: features.7.0.block.1.1.weight           requires_grad: False\n",
      "Layer: features.7.0.block.1.1.bias             requires_grad: False\n",
      "Layer: features.7.0.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.7.0.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.7.0.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.7.0.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.7.0.block.3.0.weight           requires_grad: False\n",
      "Layer: features.7.0.block.3.1.weight           requires_grad: False\n",
      "Layer: features.7.0.block.3.1.bias             requires_grad: False\n",
      "Layer: features.7.1.block.0.0.weight           requires_grad: False\n",
      "Layer: features.7.1.block.0.1.weight           requires_grad: False\n",
      "Layer: features.7.1.block.0.1.bias             requires_grad: False\n",
      "Layer: features.7.1.block.1.0.weight           requires_grad: False\n",
      "Layer: features.7.1.block.1.1.weight           requires_grad: False\n",
      "Layer: features.7.1.block.1.1.bias             requires_grad: False\n",
      "Layer: features.7.1.block.2.fc1.weight         requires_grad: False\n",
      "Layer: features.7.1.block.2.fc1.bias           requires_grad: False\n",
      "Layer: features.7.1.block.2.fc2.weight         requires_grad: False\n",
      "Layer: features.7.1.block.2.fc2.bias           requires_grad: False\n",
      "Layer: features.7.1.block.3.0.weight           requires_grad: False\n",
      "Layer: features.7.1.block.3.1.weight           requires_grad: False\n",
      "Layer: features.7.1.block.3.1.bias             requires_grad: False\n",
      "Layer: features.8.0.weight                     requires_grad: False\n",
      "Layer: features.8.1.weight                     requires_grad: False\n",
      "Layer: features.8.1.bias                       requires_grad: False\n",
      "Layer: classifier.1.weight                     requires_grad: False\n",
      "Layer: classifier.1.bias                       requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "efficientnetmodel = models.efficientnet_b4(pretrained=True)\n",
    "\n",
    "no_features = efficientnetmodel.classifier[1].in_features  \n",
    "efficientnetmodel.classifier[1] = nn.Linear(no_features, 10) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "efficientnetmodel = efficientnetmodel.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(efficientnetmodel.parameters(), lr=0.005)\n",
    "\n",
    "freeze_model(efficientnetmodel)\n",
    "showGrads(efficientnetmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training from Start\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "from datetime import datetime\n",
    "modelSaver_path = f\"modelLogs/{datetime.today().strftime('%Y-%m-%d %H-%M')}\"\n",
    "print(\"Save path:\", modelSaver_path)\n",
    "\n",
    "epochsPerPhase = 5\n",
    "phases = 4                          #? <-- this determines number of blocks to unfreeze\n",
    "modelVariation = \"type2Phases\"      #? <-- Change this each time you run a new Model or training type\n",
    "\n",
    "for phase in range(phases):\n",
    "    print(f\"Phase: {phase+1}\") \n",
    "    #? progressive unfreezing of layers\n",
    "    unfreeze_last_n_blocks(efficientnetmodel, phase)\n",
    "\n",
    "    efficientnetmodel.train()\n",
    "    for epoch in range(epochsPerPhase):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch, (inputs, labels) in enumerate(train_dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            outputs = efficientnetmodel(inputs).squeeze()\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs,1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "            print(f\" Epoch [{epoch+1}/{epochsPerPhase}]\".ljust(16),\n",
    "                  f\"Batch: {batch+1}\".ljust(10),\n",
    "                  f\"Accuracy: {((correct_train / total_train)*100):.2f}%\".ljust(20),\n",
    "                  end=\"\\r\")\n",
    "            \n",
    "        \n",
    "        train_accuracy = (correct_train / total_train)*100\n",
    "        avg_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "        print(f\" Epoch [{epoch+1}/{epochsPerPhase}]\".ljust(16),\n",
    "              f\"Loss: {avg_loss:.4f}\".ljust(10),\n",
    "              f\"Accuracy: {train_accuracy:.2f}%\".ljust(30))\n",
    "\n",
    "        os.makedirs(modelSaver_path,exist_ok=True)\n",
    "        checkpoint_filename = modelSaver_path+f\"/{modelVariation}_phase{phase+1}_epoch{epoch+1}.pth\"\n",
    "        torch.save(efficientnetmodel.state_dict(), checkpoint_filename)\n",
    "        print(f\"model saved to {checkpoint_filename}\")\n",
    "\n",
    "                \n",
    "    print(\" Evaluation \".center(30,\"-\"))\n",
    "    efficientnetmodel.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for vBatch, (inputs, labels) in enumerate(valid_dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "            outputs = efficientnetmodel(inputs).squeeze()\n",
    "            _, predicted = torch.max(outputs,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print(f\"Epoch: {epoch+1}\".ljust(10), f\"Batch: {vBatch+1}\".ljust(12), f\"Accuracy: {((correct/total)*100):.2f}%\",end=\"\\r\")\n",
    "            \n",
    "\n",
    "    val_accuracy = (correct / total)*100\n",
    "    print(f\" Validation Accuracy: {val_accuracy:.2f}%\".ljust(50))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model From Saver\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.0.weight\n",
      "features.0.1.weight\n",
      "features.0.1.bias\n",
      "features.0.1.running_mean\n",
      "features.0.1.running_var\n",
      "features.0.1.num_batches_tracked\n",
      "features.1.0.block.0.0.weight\n",
      "features.1.0.block.0.1.weight\n",
      "features.1.0.block.0.1.bias\n",
      "features.1.0.block.0.1.running_mean\n",
      "features.1.0.block.0.1.running_var\n",
      "features.1.0.block.0.1.num_batches_tracked\n",
      "features.1.0.block.1.fc1.weight\n",
      "features.1.0.block.1.fc1.bias\n",
      "features.1.0.block.1.fc2.weight\n",
      "features.1.0.block.1.fc2.bias\n",
      "features.1.0.block.2.0.weight\n",
      "features.1.0.block.2.1.weight\n",
      "features.1.0.block.2.1.bias\n",
      "features.1.0.block.2.1.running_mean\n",
      "features.1.0.block.2.1.running_var\n",
      "features.1.0.block.2.1.num_batches_tracked\n",
      "features.1.1.block.0.0.weight\n",
      "features.1.1.block.0.1.weight\n",
      "features.1.1.block.0.1.bias\n",
      "features.1.1.block.0.1.running_mean\n",
      "features.1.1.block.0.1.running_var\n",
      "features.1.1.block.0.1.num_batches_tracked\n",
      "features.1.1.block.1.fc1.weight\n",
      "features.1.1.block.1.fc1.bias\n",
      "features.1.1.block.1.fc2.weight\n",
      "features.1.1.block.1.fc2.bias\n",
      "features.1.1.block.2.0.weight\n",
      "features.1.1.block.2.1.weight\n",
      "features.1.1.block.2.1.bias\n",
      "features.1.1.block.2.1.running_mean\n",
      "features.1.1.block.2.1.running_var\n",
      "features.1.1.block.2.1.num_batches_tracked\n",
      "features.2.0.block.0.0.weight\n",
      "features.2.0.block.0.1.weight\n",
      "features.2.0.block.0.1.bias\n",
      "features.2.0.block.0.1.running_mean\n",
      "features.2.0.block.0.1.running_var\n",
      "features.2.0.block.0.1.num_batches_tracked\n",
      "features.2.0.block.1.0.weight\n",
      "features.2.0.block.1.1.weight\n",
      "features.2.0.block.1.1.bias\n",
      "features.2.0.block.1.1.running_mean\n",
      "features.2.0.block.1.1.running_var\n",
      "features.2.0.block.1.1.num_batches_tracked\n",
      "features.2.0.block.2.fc1.weight\n",
      "features.2.0.block.2.fc1.bias\n",
      "features.2.0.block.2.fc2.weight\n",
      "features.2.0.block.2.fc2.bias\n",
      "features.2.0.block.3.0.weight\n",
      "features.2.0.block.3.1.weight\n",
      "features.2.0.block.3.1.bias\n",
      "features.2.0.block.3.1.running_mean\n",
      "features.2.0.block.3.1.running_var\n",
      "features.2.0.block.3.1.num_batches_tracked\n",
      "features.2.1.block.0.0.weight\n",
      "features.2.1.block.0.1.weight\n",
      "features.2.1.block.0.1.bias\n",
      "features.2.1.block.0.1.running_mean\n",
      "features.2.1.block.0.1.running_var\n",
      "features.2.1.block.0.1.num_batches_tracked\n",
      "features.2.1.block.1.0.weight\n",
      "features.2.1.block.1.1.weight\n",
      "features.2.1.block.1.1.bias\n",
      "features.2.1.block.1.1.running_mean\n",
      "features.2.1.block.1.1.running_var\n",
      "features.2.1.block.1.1.num_batches_tracked\n",
      "features.2.1.block.2.fc1.weight\n",
      "features.2.1.block.2.fc1.bias\n",
      "features.2.1.block.2.fc2.weight\n",
      "features.2.1.block.2.fc2.bias\n",
      "features.2.1.block.3.0.weight\n",
      "features.2.1.block.3.1.weight\n",
      "features.2.1.block.3.1.bias\n",
      "features.2.1.block.3.1.running_mean\n",
      "features.2.1.block.3.1.running_var\n",
      "features.2.1.block.3.1.num_batches_tracked\n",
      "features.2.2.block.0.0.weight\n",
      "features.2.2.block.0.1.weight\n",
      "features.2.2.block.0.1.bias\n",
      "features.2.2.block.0.1.running_mean\n",
      "features.2.2.block.0.1.running_var\n",
      "features.2.2.block.0.1.num_batches_tracked\n",
      "features.2.2.block.1.0.weight\n",
      "features.2.2.block.1.1.weight\n",
      "features.2.2.block.1.1.bias\n",
      "features.2.2.block.1.1.running_mean\n",
      "features.2.2.block.1.1.running_var\n",
      "features.2.2.block.1.1.num_batches_tracked\n",
      "features.2.2.block.2.fc1.weight\n",
      "features.2.2.block.2.fc1.bias\n",
      "features.2.2.block.2.fc2.weight\n",
      "features.2.2.block.2.fc2.bias\n",
      "features.2.2.block.3.0.weight\n",
      "features.2.2.block.3.1.weight\n",
      "features.2.2.block.3.1.bias\n",
      "features.2.2.block.3.1.running_mean\n",
      "features.2.2.block.3.1.running_var\n",
      "features.2.2.block.3.1.num_batches_tracked\n",
      "features.2.3.block.0.0.weight\n",
      "features.2.3.block.0.1.weight\n",
      "features.2.3.block.0.1.bias\n",
      "features.2.3.block.0.1.running_mean\n",
      "features.2.3.block.0.1.running_var\n",
      "features.2.3.block.0.1.num_batches_tracked\n",
      "features.2.3.block.1.0.weight\n",
      "features.2.3.block.1.1.weight\n",
      "features.2.3.block.1.1.bias\n",
      "features.2.3.block.1.1.running_mean\n",
      "features.2.3.block.1.1.running_var\n",
      "features.2.3.block.1.1.num_batches_tracked\n",
      "features.2.3.block.2.fc1.weight\n",
      "features.2.3.block.2.fc1.bias\n",
      "features.2.3.block.2.fc2.weight\n",
      "features.2.3.block.2.fc2.bias\n",
      "features.2.3.block.3.0.weight\n",
      "features.2.3.block.3.1.weight\n",
      "features.2.3.block.3.1.bias\n",
      "features.2.3.block.3.1.running_mean\n",
      "features.2.3.block.3.1.running_var\n",
      "features.2.3.block.3.1.num_batches_tracked\n",
      "features.3.0.block.0.0.weight\n",
      "features.3.0.block.0.1.weight\n",
      "features.3.0.block.0.1.bias\n",
      "features.3.0.block.0.1.running_mean\n",
      "features.3.0.block.0.1.running_var\n",
      "features.3.0.block.0.1.num_batches_tracked\n",
      "features.3.0.block.1.0.weight\n",
      "features.3.0.block.1.1.weight\n",
      "features.3.0.block.1.1.bias\n",
      "features.3.0.block.1.1.running_mean\n",
      "features.3.0.block.1.1.running_var\n",
      "features.3.0.block.1.1.num_batches_tracked\n",
      "features.3.0.block.2.fc1.weight\n",
      "features.3.0.block.2.fc1.bias\n",
      "features.3.0.block.2.fc2.weight\n",
      "features.3.0.block.2.fc2.bias\n",
      "features.3.0.block.3.0.weight\n",
      "features.3.0.block.3.1.weight\n",
      "features.3.0.block.3.1.bias\n",
      "features.3.0.block.3.1.running_mean\n",
      "features.3.0.block.3.1.running_var\n",
      "features.3.0.block.3.1.num_batches_tracked\n",
      "features.3.1.block.0.0.weight\n",
      "features.3.1.block.0.1.weight\n",
      "features.3.1.block.0.1.bias\n",
      "features.3.1.block.0.1.running_mean\n",
      "features.3.1.block.0.1.running_var\n",
      "features.3.1.block.0.1.num_batches_tracked\n",
      "features.3.1.block.1.0.weight\n",
      "features.3.1.block.1.1.weight\n",
      "features.3.1.block.1.1.bias\n",
      "features.3.1.block.1.1.running_mean\n",
      "features.3.1.block.1.1.running_var\n",
      "features.3.1.block.1.1.num_batches_tracked\n",
      "features.3.1.block.2.fc1.weight\n",
      "features.3.1.block.2.fc1.bias\n",
      "features.3.1.block.2.fc2.weight\n",
      "features.3.1.block.2.fc2.bias\n",
      "features.3.1.block.3.0.weight\n",
      "features.3.1.block.3.1.weight\n",
      "features.3.1.block.3.1.bias\n",
      "features.3.1.block.3.1.running_mean\n",
      "features.3.1.block.3.1.running_var\n",
      "features.3.1.block.3.1.num_batches_tracked\n",
      "features.3.2.block.0.0.weight\n",
      "features.3.2.block.0.1.weight\n",
      "features.3.2.block.0.1.bias\n",
      "features.3.2.block.0.1.running_mean\n",
      "features.3.2.block.0.1.running_var\n",
      "features.3.2.block.0.1.num_batches_tracked\n",
      "features.3.2.block.1.0.weight\n",
      "features.3.2.block.1.1.weight\n",
      "features.3.2.block.1.1.bias\n",
      "features.3.2.block.1.1.running_mean\n",
      "features.3.2.block.1.1.running_var\n",
      "features.3.2.block.1.1.num_batches_tracked\n",
      "features.3.2.block.2.fc1.weight\n",
      "features.3.2.block.2.fc1.bias\n",
      "features.3.2.block.2.fc2.weight\n",
      "features.3.2.block.2.fc2.bias\n",
      "features.3.2.block.3.0.weight\n",
      "features.3.2.block.3.1.weight\n",
      "features.3.2.block.3.1.bias\n",
      "features.3.2.block.3.1.running_mean\n",
      "features.3.2.block.3.1.running_var\n",
      "features.3.2.block.3.1.num_batches_tracked\n",
      "features.3.3.block.0.0.weight\n",
      "features.3.3.block.0.1.weight\n",
      "features.3.3.block.0.1.bias\n",
      "features.3.3.block.0.1.running_mean\n",
      "features.3.3.block.0.1.running_var\n",
      "features.3.3.block.0.1.num_batches_tracked\n",
      "features.3.3.block.1.0.weight\n",
      "features.3.3.block.1.1.weight\n",
      "features.3.3.block.1.1.bias\n",
      "features.3.3.block.1.1.running_mean\n",
      "features.3.3.block.1.1.running_var\n",
      "features.3.3.block.1.1.num_batches_tracked\n",
      "features.3.3.block.2.fc1.weight\n",
      "features.3.3.block.2.fc1.bias\n",
      "features.3.3.block.2.fc2.weight\n",
      "features.3.3.block.2.fc2.bias\n",
      "features.3.3.block.3.0.weight\n",
      "features.3.3.block.3.1.weight\n",
      "features.3.3.block.3.1.bias\n",
      "features.3.3.block.3.1.running_mean\n",
      "features.3.3.block.3.1.running_var\n",
      "features.3.3.block.3.1.num_batches_tracked\n",
      "features.4.0.block.0.0.weight\n",
      "features.4.0.block.0.1.weight\n",
      "features.4.0.block.0.1.bias\n",
      "features.4.0.block.0.1.running_mean\n",
      "features.4.0.block.0.1.running_var\n",
      "features.4.0.block.0.1.num_batches_tracked\n",
      "features.4.0.block.1.0.weight\n",
      "features.4.0.block.1.1.weight\n",
      "features.4.0.block.1.1.bias\n",
      "features.4.0.block.1.1.running_mean\n",
      "features.4.0.block.1.1.running_var\n",
      "features.4.0.block.1.1.num_batches_tracked\n",
      "features.4.0.block.2.fc1.weight\n",
      "features.4.0.block.2.fc1.bias\n",
      "features.4.0.block.2.fc2.weight\n",
      "features.4.0.block.2.fc2.bias\n",
      "features.4.0.block.3.0.weight\n",
      "features.4.0.block.3.1.weight\n",
      "features.4.0.block.3.1.bias\n",
      "features.4.0.block.3.1.running_mean\n",
      "features.4.0.block.3.1.running_var\n",
      "features.4.0.block.3.1.num_batches_tracked\n",
      "features.4.1.block.0.0.weight\n",
      "features.4.1.block.0.1.weight\n",
      "features.4.1.block.0.1.bias\n",
      "features.4.1.block.0.1.running_mean\n",
      "features.4.1.block.0.1.running_var\n",
      "features.4.1.block.0.1.num_batches_tracked\n",
      "features.4.1.block.1.0.weight\n",
      "features.4.1.block.1.1.weight\n",
      "features.4.1.block.1.1.bias\n",
      "features.4.1.block.1.1.running_mean\n",
      "features.4.1.block.1.1.running_var\n",
      "features.4.1.block.1.1.num_batches_tracked\n",
      "features.4.1.block.2.fc1.weight\n",
      "features.4.1.block.2.fc1.bias\n",
      "features.4.1.block.2.fc2.weight\n",
      "features.4.1.block.2.fc2.bias\n",
      "features.4.1.block.3.0.weight\n",
      "features.4.1.block.3.1.weight\n",
      "features.4.1.block.3.1.bias\n",
      "features.4.1.block.3.1.running_mean\n",
      "features.4.1.block.3.1.running_var\n",
      "features.4.1.block.3.1.num_batches_tracked\n",
      "features.4.2.block.0.0.weight\n",
      "features.4.2.block.0.1.weight\n",
      "features.4.2.block.0.1.bias\n",
      "features.4.2.block.0.1.running_mean\n",
      "features.4.2.block.0.1.running_var\n",
      "features.4.2.block.0.1.num_batches_tracked\n",
      "features.4.2.block.1.0.weight\n",
      "features.4.2.block.1.1.weight\n",
      "features.4.2.block.1.1.bias\n",
      "features.4.2.block.1.1.running_mean\n",
      "features.4.2.block.1.1.running_var\n",
      "features.4.2.block.1.1.num_batches_tracked\n",
      "features.4.2.block.2.fc1.weight\n",
      "features.4.2.block.2.fc1.bias\n",
      "features.4.2.block.2.fc2.weight\n",
      "features.4.2.block.2.fc2.bias\n",
      "features.4.2.block.3.0.weight\n",
      "features.4.2.block.3.1.weight\n",
      "features.4.2.block.3.1.bias\n",
      "features.4.2.block.3.1.running_mean\n",
      "features.4.2.block.3.1.running_var\n",
      "features.4.2.block.3.1.num_batches_tracked\n",
      "features.4.3.block.0.0.weight\n",
      "features.4.3.block.0.1.weight\n",
      "features.4.3.block.0.1.bias\n",
      "features.4.3.block.0.1.running_mean\n",
      "features.4.3.block.0.1.running_var\n",
      "features.4.3.block.0.1.num_batches_tracked\n",
      "features.4.3.block.1.0.weight\n",
      "features.4.3.block.1.1.weight\n",
      "features.4.3.block.1.1.bias\n",
      "features.4.3.block.1.1.running_mean\n",
      "features.4.3.block.1.1.running_var\n",
      "features.4.3.block.1.1.num_batches_tracked\n",
      "features.4.3.block.2.fc1.weight\n",
      "features.4.3.block.2.fc1.bias\n",
      "features.4.3.block.2.fc2.weight\n",
      "features.4.3.block.2.fc2.bias\n",
      "features.4.3.block.3.0.weight\n",
      "features.4.3.block.3.1.weight\n",
      "features.4.3.block.3.1.bias\n",
      "features.4.3.block.3.1.running_mean\n",
      "features.4.3.block.3.1.running_var\n",
      "features.4.3.block.3.1.num_batches_tracked\n",
      "features.4.4.block.0.0.weight\n",
      "features.4.4.block.0.1.weight\n",
      "features.4.4.block.0.1.bias\n",
      "features.4.4.block.0.1.running_mean\n",
      "features.4.4.block.0.1.running_var\n",
      "features.4.4.block.0.1.num_batches_tracked\n",
      "features.4.4.block.1.0.weight\n",
      "features.4.4.block.1.1.weight\n",
      "features.4.4.block.1.1.bias\n",
      "features.4.4.block.1.1.running_mean\n",
      "features.4.4.block.1.1.running_var\n",
      "features.4.4.block.1.1.num_batches_tracked\n",
      "features.4.4.block.2.fc1.weight\n",
      "features.4.4.block.2.fc1.bias\n",
      "features.4.4.block.2.fc2.weight\n",
      "features.4.4.block.2.fc2.bias\n",
      "features.4.4.block.3.0.weight\n",
      "features.4.4.block.3.1.weight\n",
      "features.4.4.block.3.1.bias\n",
      "features.4.4.block.3.1.running_mean\n",
      "features.4.4.block.3.1.running_var\n",
      "features.4.4.block.3.1.num_batches_tracked\n",
      "features.4.5.block.0.0.weight\n",
      "features.4.5.block.0.1.weight\n",
      "features.4.5.block.0.1.bias\n",
      "features.4.5.block.0.1.running_mean\n",
      "features.4.5.block.0.1.running_var\n",
      "features.4.5.block.0.1.num_batches_tracked\n",
      "features.4.5.block.1.0.weight\n",
      "features.4.5.block.1.1.weight\n",
      "features.4.5.block.1.1.bias\n",
      "features.4.5.block.1.1.running_mean\n",
      "features.4.5.block.1.1.running_var\n",
      "features.4.5.block.1.1.num_batches_tracked\n",
      "features.4.5.block.2.fc1.weight\n",
      "features.4.5.block.2.fc1.bias\n",
      "features.4.5.block.2.fc2.weight\n",
      "features.4.5.block.2.fc2.bias\n",
      "features.4.5.block.3.0.weight\n",
      "features.4.5.block.3.1.weight\n",
      "features.4.5.block.3.1.bias\n",
      "features.4.5.block.3.1.running_mean\n",
      "features.4.5.block.3.1.running_var\n",
      "features.4.5.block.3.1.num_batches_tracked\n",
      "features.5.0.block.0.0.weight\n",
      "features.5.0.block.0.1.weight\n",
      "features.5.0.block.0.1.bias\n",
      "features.5.0.block.0.1.running_mean\n",
      "features.5.0.block.0.1.running_var\n",
      "features.5.0.block.0.1.num_batches_tracked\n",
      "features.5.0.block.1.0.weight\n",
      "features.5.0.block.1.1.weight\n",
      "features.5.0.block.1.1.bias\n",
      "features.5.0.block.1.1.running_mean\n",
      "features.5.0.block.1.1.running_var\n",
      "features.5.0.block.1.1.num_batches_tracked\n",
      "features.5.0.block.2.fc1.weight\n",
      "features.5.0.block.2.fc1.bias\n",
      "features.5.0.block.2.fc2.weight\n",
      "features.5.0.block.2.fc2.bias\n",
      "features.5.0.block.3.0.weight\n",
      "features.5.0.block.3.1.weight\n",
      "features.5.0.block.3.1.bias\n",
      "features.5.0.block.3.1.running_mean\n",
      "features.5.0.block.3.1.running_var\n",
      "features.5.0.block.3.1.num_batches_tracked\n",
      "features.5.1.block.0.0.weight\n",
      "features.5.1.block.0.1.weight\n",
      "features.5.1.block.0.1.bias\n",
      "features.5.1.block.0.1.running_mean\n",
      "features.5.1.block.0.1.running_var\n",
      "features.5.1.block.0.1.num_batches_tracked\n",
      "features.5.1.block.1.0.weight\n",
      "features.5.1.block.1.1.weight\n",
      "features.5.1.block.1.1.bias\n",
      "features.5.1.block.1.1.running_mean\n",
      "features.5.1.block.1.1.running_var\n",
      "features.5.1.block.1.1.num_batches_tracked\n",
      "features.5.1.block.2.fc1.weight\n",
      "features.5.1.block.2.fc1.bias\n",
      "features.5.1.block.2.fc2.weight\n",
      "features.5.1.block.2.fc2.bias\n",
      "features.5.1.block.3.0.weight\n",
      "features.5.1.block.3.1.weight\n",
      "features.5.1.block.3.1.bias\n",
      "features.5.1.block.3.1.running_mean\n",
      "features.5.1.block.3.1.running_var\n",
      "features.5.1.block.3.1.num_batches_tracked\n",
      "features.5.2.block.0.0.weight\n",
      "features.5.2.block.0.1.weight\n",
      "features.5.2.block.0.1.bias\n",
      "features.5.2.block.0.1.running_mean\n",
      "features.5.2.block.0.1.running_var\n",
      "features.5.2.block.0.1.num_batches_tracked\n",
      "features.5.2.block.1.0.weight\n",
      "features.5.2.block.1.1.weight\n",
      "features.5.2.block.1.1.bias\n",
      "features.5.2.block.1.1.running_mean\n",
      "features.5.2.block.1.1.running_var\n",
      "features.5.2.block.1.1.num_batches_tracked\n",
      "features.5.2.block.2.fc1.weight\n",
      "features.5.2.block.2.fc1.bias\n",
      "features.5.2.block.2.fc2.weight\n",
      "features.5.2.block.2.fc2.bias\n",
      "features.5.2.block.3.0.weight\n",
      "features.5.2.block.3.1.weight\n",
      "features.5.2.block.3.1.bias\n",
      "features.5.2.block.3.1.running_mean\n",
      "features.5.2.block.3.1.running_var\n",
      "features.5.2.block.3.1.num_batches_tracked\n",
      "features.5.3.block.0.0.weight\n",
      "features.5.3.block.0.1.weight\n",
      "features.5.3.block.0.1.bias\n",
      "features.5.3.block.0.1.running_mean\n",
      "features.5.3.block.0.1.running_var\n",
      "features.5.3.block.0.1.num_batches_tracked\n",
      "features.5.3.block.1.0.weight\n",
      "features.5.3.block.1.1.weight\n",
      "features.5.3.block.1.1.bias\n",
      "features.5.3.block.1.1.running_mean\n",
      "features.5.3.block.1.1.running_var\n",
      "features.5.3.block.1.1.num_batches_tracked\n",
      "features.5.3.block.2.fc1.weight\n",
      "features.5.3.block.2.fc1.bias\n",
      "features.5.3.block.2.fc2.weight\n",
      "features.5.3.block.2.fc2.bias\n",
      "features.5.3.block.3.0.weight\n",
      "features.5.3.block.3.1.weight\n",
      "features.5.3.block.3.1.bias\n",
      "features.5.3.block.3.1.running_mean\n",
      "features.5.3.block.3.1.running_var\n",
      "features.5.3.block.3.1.num_batches_tracked\n",
      "features.5.4.block.0.0.weight\n",
      "features.5.4.block.0.1.weight\n",
      "features.5.4.block.0.1.bias\n",
      "features.5.4.block.0.1.running_mean\n",
      "features.5.4.block.0.1.running_var\n",
      "features.5.4.block.0.1.num_batches_tracked\n",
      "features.5.4.block.1.0.weight\n",
      "features.5.4.block.1.1.weight\n",
      "features.5.4.block.1.1.bias\n",
      "features.5.4.block.1.1.running_mean\n",
      "features.5.4.block.1.1.running_var\n",
      "features.5.4.block.1.1.num_batches_tracked\n",
      "features.5.4.block.2.fc1.weight\n",
      "features.5.4.block.2.fc1.bias\n",
      "features.5.4.block.2.fc2.weight\n",
      "features.5.4.block.2.fc2.bias\n",
      "features.5.4.block.3.0.weight\n",
      "features.5.4.block.3.1.weight\n",
      "features.5.4.block.3.1.bias\n",
      "features.5.4.block.3.1.running_mean\n",
      "features.5.4.block.3.1.running_var\n",
      "features.5.4.block.3.1.num_batches_tracked\n",
      "features.5.5.block.0.0.weight\n",
      "features.5.5.block.0.1.weight\n",
      "features.5.5.block.0.1.bias\n",
      "features.5.5.block.0.1.running_mean\n",
      "features.5.5.block.0.1.running_var\n",
      "features.5.5.block.0.1.num_batches_tracked\n",
      "features.5.5.block.1.0.weight\n",
      "features.5.5.block.1.1.weight\n",
      "features.5.5.block.1.1.bias\n",
      "features.5.5.block.1.1.running_mean\n",
      "features.5.5.block.1.1.running_var\n",
      "features.5.5.block.1.1.num_batches_tracked\n",
      "features.5.5.block.2.fc1.weight\n",
      "features.5.5.block.2.fc1.bias\n",
      "features.5.5.block.2.fc2.weight\n",
      "features.5.5.block.2.fc2.bias\n",
      "features.5.5.block.3.0.weight\n",
      "features.5.5.block.3.1.weight\n",
      "features.5.5.block.3.1.bias\n",
      "features.5.5.block.3.1.running_mean\n",
      "features.5.5.block.3.1.running_var\n",
      "features.5.5.block.3.1.num_batches_tracked\n",
      "features.6.0.block.0.0.weight\n",
      "features.6.0.block.0.1.weight\n",
      "features.6.0.block.0.1.bias\n",
      "features.6.0.block.0.1.running_mean\n",
      "features.6.0.block.0.1.running_var\n",
      "features.6.0.block.0.1.num_batches_tracked\n",
      "features.6.0.block.1.0.weight\n",
      "features.6.0.block.1.1.weight\n",
      "features.6.0.block.1.1.bias\n",
      "features.6.0.block.1.1.running_mean\n",
      "features.6.0.block.1.1.running_var\n",
      "features.6.0.block.1.1.num_batches_tracked\n",
      "features.6.0.block.2.fc1.weight\n",
      "features.6.0.block.2.fc1.bias\n",
      "features.6.0.block.2.fc2.weight\n",
      "features.6.0.block.2.fc2.bias\n",
      "features.6.0.block.3.0.weight\n",
      "features.6.0.block.3.1.weight\n",
      "features.6.0.block.3.1.bias\n",
      "features.6.0.block.3.1.running_mean\n",
      "features.6.0.block.3.1.running_var\n",
      "features.6.0.block.3.1.num_batches_tracked\n",
      "features.6.1.block.0.0.weight\n",
      "features.6.1.block.0.1.weight\n",
      "features.6.1.block.0.1.bias\n",
      "features.6.1.block.0.1.running_mean\n",
      "features.6.1.block.0.1.running_var\n",
      "features.6.1.block.0.1.num_batches_tracked\n",
      "features.6.1.block.1.0.weight\n",
      "features.6.1.block.1.1.weight\n",
      "features.6.1.block.1.1.bias\n",
      "features.6.1.block.1.1.running_mean\n",
      "features.6.1.block.1.1.running_var\n",
      "features.6.1.block.1.1.num_batches_tracked\n",
      "features.6.1.block.2.fc1.weight\n",
      "features.6.1.block.2.fc1.bias\n",
      "features.6.1.block.2.fc2.weight\n",
      "features.6.1.block.2.fc2.bias\n",
      "features.6.1.block.3.0.weight\n",
      "features.6.1.block.3.1.weight\n",
      "features.6.1.block.3.1.bias\n",
      "features.6.1.block.3.1.running_mean\n",
      "features.6.1.block.3.1.running_var\n",
      "features.6.1.block.3.1.num_batches_tracked\n",
      "features.6.2.block.0.0.weight\n",
      "features.6.2.block.0.1.weight\n",
      "features.6.2.block.0.1.bias\n",
      "features.6.2.block.0.1.running_mean\n",
      "features.6.2.block.0.1.running_var\n",
      "features.6.2.block.0.1.num_batches_tracked\n",
      "features.6.2.block.1.0.weight\n",
      "features.6.2.block.1.1.weight\n",
      "features.6.2.block.1.1.bias\n",
      "features.6.2.block.1.1.running_mean\n",
      "features.6.2.block.1.1.running_var\n",
      "features.6.2.block.1.1.num_batches_tracked\n",
      "features.6.2.block.2.fc1.weight\n",
      "features.6.2.block.2.fc1.bias\n",
      "features.6.2.block.2.fc2.weight\n",
      "features.6.2.block.2.fc2.bias\n",
      "features.6.2.block.3.0.weight\n",
      "features.6.2.block.3.1.weight\n",
      "features.6.2.block.3.1.bias\n",
      "features.6.2.block.3.1.running_mean\n",
      "features.6.2.block.3.1.running_var\n",
      "features.6.2.block.3.1.num_batches_tracked\n",
      "features.6.3.block.0.0.weight\n",
      "features.6.3.block.0.1.weight\n",
      "features.6.3.block.0.1.bias\n",
      "features.6.3.block.0.1.running_mean\n",
      "features.6.3.block.0.1.running_var\n",
      "features.6.3.block.0.1.num_batches_tracked\n",
      "features.6.3.block.1.0.weight\n",
      "features.6.3.block.1.1.weight\n",
      "features.6.3.block.1.1.bias\n",
      "features.6.3.block.1.1.running_mean\n",
      "features.6.3.block.1.1.running_var\n",
      "features.6.3.block.1.1.num_batches_tracked\n",
      "features.6.3.block.2.fc1.weight\n",
      "features.6.3.block.2.fc1.bias\n",
      "features.6.3.block.2.fc2.weight\n",
      "features.6.3.block.2.fc2.bias\n",
      "features.6.3.block.3.0.weight\n",
      "features.6.3.block.3.1.weight\n",
      "features.6.3.block.3.1.bias\n",
      "features.6.3.block.3.1.running_mean\n",
      "features.6.3.block.3.1.running_var\n",
      "features.6.3.block.3.1.num_batches_tracked\n",
      "features.6.4.block.0.0.weight\n",
      "features.6.4.block.0.1.weight\n",
      "features.6.4.block.0.1.bias\n",
      "features.6.4.block.0.1.running_mean\n",
      "features.6.4.block.0.1.running_var\n",
      "features.6.4.block.0.1.num_batches_tracked\n",
      "features.6.4.block.1.0.weight\n",
      "features.6.4.block.1.1.weight\n",
      "features.6.4.block.1.1.bias\n",
      "features.6.4.block.1.1.running_mean\n",
      "features.6.4.block.1.1.running_var\n",
      "features.6.4.block.1.1.num_batches_tracked\n",
      "features.6.4.block.2.fc1.weight\n",
      "features.6.4.block.2.fc1.bias\n",
      "features.6.4.block.2.fc2.weight\n",
      "features.6.4.block.2.fc2.bias\n",
      "features.6.4.block.3.0.weight\n",
      "features.6.4.block.3.1.weight\n",
      "features.6.4.block.3.1.bias\n",
      "features.6.4.block.3.1.running_mean\n",
      "features.6.4.block.3.1.running_var\n",
      "features.6.4.block.3.1.num_batches_tracked\n",
      "features.6.5.block.0.0.weight\n",
      "features.6.5.block.0.1.weight\n",
      "features.6.5.block.0.1.bias\n",
      "features.6.5.block.0.1.running_mean\n",
      "features.6.5.block.0.1.running_var\n",
      "features.6.5.block.0.1.num_batches_tracked\n",
      "features.6.5.block.1.0.weight\n",
      "features.6.5.block.1.1.weight\n",
      "features.6.5.block.1.1.bias\n",
      "features.6.5.block.1.1.running_mean\n",
      "features.6.5.block.1.1.running_var\n",
      "features.6.5.block.1.1.num_batches_tracked\n",
      "features.6.5.block.2.fc1.weight\n",
      "features.6.5.block.2.fc1.bias\n",
      "features.6.5.block.2.fc2.weight\n",
      "features.6.5.block.2.fc2.bias\n",
      "features.6.5.block.3.0.weight\n",
      "features.6.5.block.3.1.weight\n",
      "features.6.5.block.3.1.bias\n",
      "features.6.5.block.3.1.running_mean\n",
      "features.6.5.block.3.1.running_var\n",
      "features.6.5.block.3.1.num_batches_tracked\n",
      "features.6.6.block.0.0.weight\n",
      "features.6.6.block.0.1.weight\n",
      "features.6.6.block.0.1.bias\n",
      "features.6.6.block.0.1.running_mean\n",
      "features.6.6.block.0.1.running_var\n",
      "features.6.6.block.0.1.num_batches_tracked\n",
      "features.6.6.block.1.0.weight\n",
      "features.6.6.block.1.1.weight\n",
      "features.6.6.block.1.1.bias\n",
      "features.6.6.block.1.1.running_mean\n",
      "features.6.6.block.1.1.running_var\n",
      "features.6.6.block.1.1.num_batches_tracked\n",
      "features.6.6.block.2.fc1.weight\n",
      "features.6.6.block.2.fc1.bias\n",
      "features.6.6.block.2.fc2.weight\n",
      "features.6.6.block.2.fc2.bias\n",
      "features.6.6.block.3.0.weight\n",
      "features.6.6.block.3.1.weight\n",
      "features.6.6.block.3.1.bias\n",
      "features.6.6.block.3.1.running_mean\n",
      "features.6.6.block.3.1.running_var\n",
      "features.6.6.block.3.1.num_batches_tracked\n",
      "features.6.7.block.0.0.weight\n",
      "features.6.7.block.0.1.weight\n",
      "features.6.7.block.0.1.bias\n",
      "features.6.7.block.0.1.running_mean\n",
      "features.6.7.block.0.1.running_var\n",
      "features.6.7.block.0.1.num_batches_tracked\n",
      "features.6.7.block.1.0.weight\n",
      "features.6.7.block.1.1.weight\n",
      "features.6.7.block.1.1.bias\n",
      "features.6.7.block.1.1.running_mean\n",
      "features.6.7.block.1.1.running_var\n",
      "features.6.7.block.1.1.num_batches_tracked\n",
      "features.6.7.block.2.fc1.weight\n",
      "features.6.7.block.2.fc1.bias\n",
      "features.6.7.block.2.fc2.weight\n",
      "features.6.7.block.2.fc2.bias\n",
      "features.6.7.block.3.0.weight\n",
      "features.6.7.block.3.1.weight\n",
      "features.6.7.block.3.1.bias\n",
      "features.6.7.block.3.1.running_mean\n",
      "features.6.7.block.3.1.running_var\n",
      "features.6.7.block.3.1.num_batches_tracked\n",
      "features.7.0.block.0.0.weight\n",
      "features.7.0.block.0.1.weight\n",
      "features.7.0.block.0.1.bias\n",
      "features.7.0.block.0.1.running_mean\n",
      "features.7.0.block.0.1.running_var\n",
      "features.7.0.block.0.1.num_batches_tracked\n",
      "features.7.0.block.1.0.weight\n",
      "features.7.0.block.1.1.weight\n",
      "features.7.0.block.1.1.bias\n",
      "features.7.0.block.1.1.running_mean\n",
      "features.7.0.block.1.1.running_var\n",
      "features.7.0.block.1.1.num_batches_tracked\n",
      "features.7.0.block.2.fc1.weight\n",
      "features.7.0.block.2.fc1.bias\n",
      "features.7.0.block.2.fc2.weight\n",
      "features.7.0.block.2.fc2.bias\n",
      "features.7.0.block.3.0.weight\n",
      "features.7.0.block.3.1.weight\n",
      "features.7.0.block.3.1.bias\n",
      "features.7.0.block.3.1.running_mean\n",
      "features.7.0.block.3.1.running_var\n",
      "features.7.0.block.3.1.num_batches_tracked\n",
      "features.7.1.block.0.0.weight\n",
      "features.7.1.block.0.1.weight\n",
      "features.7.1.block.0.1.bias\n",
      "features.7.1.block.0.1.running_mean\n",
      "features.7.1.block.0.1.running_var\n",
      "features.7.1.block.0.1.num_batches_tracked\n",
      "features.7.1.block.1.0.weight\n",
      "features.7.1.block.1.1.weight\n",
      "features.7.1.block.1.1.bias\n",
      "features.7.1.block.1.1.running_mean\n",
      "features.7.1.block.1.1.running_var\n",
      "features.7.1.block.1.1.num_batches_tracked\n",
      "features.7.1.block.2.fc1.weight\n",
      "features.7.1.block.2.fc1.bias\n",
      "features.7.1.block.2.fc2.weight\n",
      "features.7.1.block.2.fc2.bias\n",
      "features.7.1.block.3.0.weight\n",
      "features.7.1.block.3.1.weight\n",
      "features.7.1.block.3.1.bias\n",
      "features.7.1.block.3.1.running_mean\n",
      "features.7.1.block.3.1.running_var\n",
      "features.7.1.block.3.1.num_batches_tracked\n",
      "features.8.0.weight\n",
      "features.8.1.weight\n",
      "features.8.1.bias\n",
      "features.8.1.running_mean\n",
      "features.8.1.running_var\n",
      "features.8.1.num_batches_tracked\n",
      "classifier.1.weight\n",
      "classifier.1.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "modelSaver_path = \"modelLogs/2025-03-28 02-15\"\n",
    "modelVariation = \"type2Phases\"\n",
    "checkpoint = torch.load(f\"{modelSaver_path}/{modelVariation}_phase4_epoch1.pth\",weights_only=False)\n",
    "\n",
    "for key in list(checkpoint.keys()):\n",
    "    print(key)\n",
    "\n",
    "\n",
    "efficientnetmodel.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train From Saver\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: 5\n",
      "Layers unfrozen: ['classifier', 'features.8', 'features.7', 'features.6', 'features.5']\n",
      " Epoch [1/5]     Batch: 1   Accuracy: 84.38%    \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.56 GiB is allocated by PyTorch, and 216.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     30\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mefficientnetmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:343\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:333\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 333\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    336\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.56 GiB is allocated by PyTorch, and 216.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "epochsPerPhase = 5\n",
    "resumePhase = 4  #? <- refer to saveFile\n",
    "phases = 5\n",
    "\n",
    "\n",
    "for phase in range(resumePhase,phases):\n",
    "    print(f\"Phase: {phase+1}\") \n",
    "    #? progressive unfreezing of layers\n",
    "    unfreeze_last_n_blocks(efficientnetmodel, phase)\n",
    "\n",
    "    efficientnetmodel.train()\n",
    "    for epoch in range(epochsPerPhase):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch, (inputs, labels) in enumerate(train_dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            outputs = efficientnetmodel(inputs).squeeze()\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs,1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "            print(f\" Epoch [{epoch+1}/{epochsPerPhase}]\".ljust(16),\n",
    "                  f\"Batch: {batch+1}\".ljust(10),\n",
    "                  f\"Accuracy: {((correct_train / total_train)*100):.2f}%\".ljust(20),\n",
    "                  end=\"\\r\")\n",
    "            \n",
    "        \n",
    "        train_accuracy = (correct_train / total_train)*100\n",
    "        avg_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "        print(f\" Epoch [{epoch+1}/{epochsPerPhase}]\".ljust(16),\n",
    "              f\"Loss: {avg_loss:.4f}\".ljust(10),\n",
    "              f\"Accuracy: {train_accuracy:.2f}%\".ljust(30))\n",
    "\n",
    "        os.makedirs(modelSaver_path,exist_ok=True)\n",
    "        checkpoint_filename = modelSaver_path+f\"/{modelVariation}_phase{phase+1}_epoch{epoch+1}.pth\"\n",
    "        torch.save(efficientnetmodel.state_dict(), checkpoint_filename)\n",
    "        print(f\"model saved to {checkpoint_filename}\")\n",
    "\n",
    "                \n",
    "    print(\" Evaluation \".center(30,\"-\"))\n",
    "    efficientnetmodel.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for vBatch, (inputs, labels) in enumerate(valid_dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "            outputs = efficientnetmodel(inputs).squeeze()\n",
    "            _, predicted = torch.max(outputs,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print(f\"Epoch: {epoch+1}\".ljust(10), f\"Batch: {vBatch+1}\".ljust(12), f\"Accuracy: {((correct/total)*100):.2f}%\",end=\"\\r\")\n",
    "            \n",
    "\n",
    "    val_accuracy = (correct / total)*100\n",
    "    print(f\" Validation Accuracy: {val_accuracy:.2f}%\".ljust(50))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Testing -----------\n",
      " Validation Accuracy: 71.20%                      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Testing \".center(30,\"-\"))\n",
    "efficientnetmodel.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for tBatch, (inputs, labels) in enumerate(test_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "        outputs = efficientnetmodel(inputs).squeeze()\n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Batch: {tBatch+1}\".ljust(12), f\"Accuracy: {((correct/total)*100):.2f}%\",end=\"\\r\")\n",
    "        \n",
    "\n",
    "val_accuracy = (correct / total)*100\n",
    "print(f\" Validation Accuracy: {val_accuracy:.2f}%\".ljust(50))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
